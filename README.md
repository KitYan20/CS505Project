
## Harry Potter's Adventure With GPT2 (Generative Pre-trained Transformer 2) And LSTM (Long short-term memory)!
![Description of the image](harrypotter.png)

This was a group project for our class (CS505) at Boston University that me and one other teammate worked on to generate large Harry Potter novels finetuning GPT-2 transformer model and designing a LSTM model.

Tech Stack: Python(PyTorch), Jupyter Notebook
## Feel free to explore the Jupyter Notebook of the code we wrote to generate Harry Potter Text!

## GPT-2 (Generative Pre-trained Transformer 2) Harry Potter Generated Text
![Description of the image](GPT-2.png)
## LSTM(Long short-term memory) Recurrent Neural Network Harry Potter Generated Text Model
![Description of the image](LSTM.png)
